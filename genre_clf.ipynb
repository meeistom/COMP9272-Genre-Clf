{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./dataset/data.csv')\n",
    "df['text'] = df['title'] + df['summary']\n",
    "\n",
    "df.drop(columns=['index', 'title', 'summary'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fantasy</td>\n",
       "      <td>Drowned Wednesday Drowned Wednesday is the fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fantasy</td>\n",
       "      <td>The Lost Hero As the book opens, Jason awakens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fantasy</td>\n",
       "      <td>The Eyes of the Overworld Cugel is easily pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fantasy</td>\n",
       "      <td>Magic's Promise The book opens with Herald-Mag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fantasy</td>\n",
       "      <td>Taran Wanderer Taran and Gurgi have returned t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     genre                                               text\n",
       "0  fantasy  Drowned Wednesday Drowned Wednesday is the fir...\n",
       "1  fantasy  The Lost Hero As the book opens, Jason awakens...\n",
       "2  fantasy  The Eyes of the Overworld Cugel is easily pers...\n",
       "3  fantasy  Magic's Promise The book opens with Herald-Mag...\n",
       "4  fantasy  Taran Wanderer Taran and Gurgi have returned t..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4657 entries, 0 to 4656\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   genre   4657 non-null   object\n",
      " 1   text    4657 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 72.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4657, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['fantasy', 'science', 'crime', 'history', 'horror', 'thriller',\n",
       "        'psychology', 'romance', 'sports', 'travel'], dtype=object),\n",
       " (10,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = df['genre'].unique()\n",
    "classes, classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fantasy': 876, 'science': 647, 'crime': 500, 'history': 600, 'horror': 600, 'thriller': 1023, 'psychology': 100, 'romance': 111, 'sports': 100, 'travel': 100}\n"
     ]
    }
   ],
   "source": [
    "classes_count_dct = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['genre'] in classes_count_dct:\n",
    "        classes_count_dct[row['genre']] += 1\n",
    "    else:\n",
    "        classes_count_dct[row['genre']] = 1\n",
    "\n",
    "print(classes_count_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [ps.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/tom/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def max_dct_count(dct):\n",
    "    return max(dct.values())\n",
    "\n",
    "def calc_alphas(classes, max_count, count_dct):\n",
    "    return {cls: 1 - count_dct[cls]/max_count for cls in classes}\n",
    "\n",
    "def join_lists(lst):\n",
    "    final = []\n",
    "    for l in lst:\n",
    "        final.extend(l)\n",
    "    return final\n",
    "\n",
    "def word_replacement(genre, text):\n",
    "    final = ''\n",
    "    for word in text.split():\n",
    "        synonyms = join_lists(wordnet.synonyms(word))\n",
    "        \n",
    "        choice = word\n",
    "\n",
    "        if synonyms and len(word) >= 3:\n",
    "            synonyms = synonyms[0]\n",
    "            threshold = 0.7 # 70% of words will be changed\n",
    "            choice = random.choice(synonyms).lower() if random.uniform(0, 1) < threshold else choice\n",
    "\n",
    "        final += choice + ' '\n",
    "\n",
    "    return {'genre': genre, 'text': final.strip()} \n",
    "\n",
    "def data_aug(df, count_dct, classes):\n",
    "    max_count = max_dct_count(count_dct)\n",
    "    alphas_dct = calc_alphas(classes, max_count, count_dct)\n",
    "    total_added = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        cls = row['genre']\n",
    "\n",
    "        while alphas_dct[cls] > random.uniform(0, 1) and count_dct[cls] < max_count:\n",
    "            df = df._append(word_replacement(cls, row['text']), ignore_index=True)\n",
    "            total_added += 1\n",
    "            count_dct[cls] += 1\n",
    "\n",
    "        if index % 10 == 0:\n",
    "            print(f'{index} samples augmented, {total_added} new samples added')\n",
    "\n",
    "    print('done')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 samples augmented, 1 new samples added\n",
      "10 samples augmented, 4 new samples added\n",
      "20 samples augmented, 7 new samples added\n",
      "30 samples augmented, 7 new samples added\n",
      "40 samples augmented, 8 new samples added\n",
      "50 samples augmented, 9 new samples added\n",
      "60 samples augmented, 10 new samples added\n",
      "70 samples augmented, 11 new samples added\n",
      "80 samples augmented, 13 new samples added\n",
      "90 samples augmented, 13 new samples added\n",
      "100 samples augmented, 13 new samples added\n",
      "110 samples augmented, 14 new samples added\n",
      "120 samples augmented, 16 new samples added\n",
      "130 samples augmented, 17 new samples added\n",
      "140 samples augmented, 19 new samples added\n",
      "150 samples augmented, 21 new samples added\n",
      "160 samples augmented, 23 new samples added\n",
      "170 samples augmented, 25 new samples added\n",
      "180 samples augmented, 26 new samples added\n",
      "190 samples augmented, 31 new samples added\n",
      "200 samples augmented, 34 new samples added\n",
      "210 samples augmented, 35 new samples added\n",
      "220 samples augmented, 38 new samples added\n",
      "230 samples augmented, 42 new samples added\n",
      "240 samples augmented, 46 new samples added\n",
      "250 samples augmented, 48 new samples added\n",
      "260 samples augmented, 48 new samples added\n",
      "270 samples augmented, 48 new samples added\n",
      "280 samples augmented, 51 new samples added\n",
      "290 samples augmented, 52 new samples added\n",
      "300 samples augmented, 52 new samples added\n",
      "310 samples augmented, 54 new samples added\n",
      "320 samples augmented, 55 new samples added\n",
      "330 samples augmented, 56 new samples added\n",
      "340 samples augmented, 58 new samples added\n",
      "350 samples augmented, 59 new samples added\n",
      "360 samples augmented, 63 new samples added\n",
      "370 samples augmented, 66 new samples added\n",
      "380 samples augmented, 66 new samples added\n",
      "390 samples augmented, 66 new samples added\n",
      "400 samples augmented, 68 new samples added\n",
      "410 samples augmented, 70 new samples added\n",
      "420 samples augmented, 71 new samples added\n",
      "430 samples augmented, 72 new samples added\n",
      "440 samples augmented, 74 new samples added\n",
      "450 samples augmented, 76 new samples added\n",
      "460 samples augmented, 79 new samples added\n",
      "470 samples augmented, 82 new samples added\n",
      "480 samples augmented, 85 new samples added\n",
      "490 samples augmented, 85 new samples added\n",
      "500 samples augmented, 86 new samples added\n",
      "510 samples augmented, 94 new samples added\n",
      "520 samples augmented, 105 new samples added\n",
      "530 samples augmented, 112 new samples added\n",
      "540 samples augmented, 120 new samples added\n",
      "550 samples augmented, 129 new samples added\n",
      "560 samples augmented, 133 new samples added\n",
      "570 samples augmented, 135 new samples added\n",
      "580 samples augmented, 141 new samples added\n",
      "590 samples augmented, 151 new samples added\n",
      "600 samples augmented, 157 new samples added\n",
      "610 samples augmented, 165 new samples added\n",
      "620 samples augmented, 172 new samples added\n",
      "630 samples augmented, 177 new samples added\n",
      "640 samples augmented, 183 new samples added\n",
      "650 samples augmented, 190 new samples added\n",
      "660 samples augmented, 193 new samples added\n",
      "670 samples augmented, 201 new samples added\n",
      "680 samples augmented, 207 new samples added\n",
      "690 samples augmented, 211 new samples added\n",
      "700 samples augmented, 214 new samples added\n",
      "710 samples augmented, 220 new samples added\n",
      "720 samples augmented, 221 new samples added\n",
      "730 samples augmented, 224 new samples added\n",
      "740 samples augmented, 229 new samples added\n",
      "750 samples augmented, 237 new samples added\n",
      "760 samples augmented, 241 new samples added\n",
      "770 samples augmented, 248 new samples added\n",
      "780 samples augmented, 253 new samples added\n",
      "790 samples augmented, 256 new samples added\n",
      "800 samples augmented, 263 new samples added\n",
      "810 samples augmented, 268 new samples added\n",
      "820 samples augmented, 274 new samples added\n",
      "830 samples augmented, 283 new samples added\n",
      "840 samples augmented, 291 new samples added\n",
      "850 samples augmented, 299 new samples added\n",
      "860 samples augmented, 307 new samples added\n",
      "870 samples augmented, 314 new samples added\n",
      "880 samples augmented, 319 new samples added\n",
      "890 samples augmented, 327 new samples added\n",
      "900 samples augmented, 335 new samples added\n",
      "910 samples augmented, 337 new samples added\n",
      "920 samples augmented, 341 new samples added\n",
      "930 samples augmented, 352 new samples added\n",
      "940 samples augmented, 355 new samples added\n",
      "950 samples augmented, 363 new samples added\n",
      "960 samples augmented, 367 new samples added\n",
      "970 samples augmented, 381 new samples added\n",
      "980 samples augmented, 383 new samples added\n",
      "990 samples augmented, 387 new samples added\n",
      "1000 samples augmented, 397 new samples added\n",
      "1010 samples augmented, 408 new samples added\n",
      "1020 samples augmented, 429 new samples added\n",
      "1030 samples augmented, 441 new samples added\n",
      "1040 samples augmented, 455 new samples added\n",
      "1050 samples augmented, 467 new samples added\n",
      "1060 samples augmented, 480 new samples added\n",
      "1070 samples augmented, 488 new samples added\n",
      "1080 samples augmented, 496 new samples added\n",
      "1090 samples augmented, 504 new samples added\n",
      "1100 samples augmented, 509 new samples added\n",
      "1110 samples augmented, 515 new samples added\n",
      "1120 samples augmented, 525 new samples added\n",
      "1130 samples augmented, 536 new samples added\n",
      "1140 samples augmented, 550 new samples added\n",
      "1150 samples augmented, 563 new samples added\n",
      "1160 samples augmented, 575 new samples added\n",
      "1170 samples augmented, 589 new samples added\n",
      "1180 samples augmented, 596 new samples added\n",
      "1190 samples augmented, 599 new samples added\n",
      "1200 samples augmented, 608 new samples added\n",
      "1210 samples augmented, 625 new samples added\n",
      "1220 samples augmented, 638 new samples added\n",
      "1230 samples augmented, 656 new samples added\n",
      "1240 samples augmented, 667 new samples added\n",
      "1250 samples augmented, 677 new samples added\n",
      "1260 samples augmented, 693 new samples added\n",
      "1270 samples augmented, 700 new samples added\n",
      "1280 samples augmented, 707 new samples added\n",
      "1290 samples augmented, 714 new samples added\n",
      "1300 samples augmented, 731 new samples added\n",
      "1310 samples augmented, 744 new samples added\n",
      "1320 samples augmented, 755 new samples added\n",
      "1330 samples augmented, 766 new samples added\n",
      "1340 samples augmented, 777 new samples added\n",
      "1350 samples augmented, 788 new samples added\n",
      "1360 samples augmented, 801 new samples added\n",
      "1370 samples augmented, 805 new samples added\n",
      "1380 samples augmented, 810 new samples added\n",
      "1390 samples augmented, 818 new samples added\n",
      "1400 samples augmented, 837 new samples added\n",
      "1410 samples augmented, 846 new samples added\n",
      "1420 samples augmented, 856 new samples added\n",
      "1430 samples augmented, 864 new samples added\n",
      "1440 samples augmented, 868 new samples added\n",
      "1450 samples augmented, 878 new samples added\n",
      "1460 samples augmented, 881 new samples added\n",
      "1470 samples augmented, 895 new samples added\n",
      "1480 samples augmented, 914 new samples added\n",
      "1490 samples augmented, 917 new samples added\n",
      "1500 samples augmented, 917 new samples added\n",
      "1510 samples augmented, 921 new samples added\n",
      "1520 samples augmented, 928 new samples added\n",
      "1530 samples augmented, 932 new samples added\n",
      "1540 samples augmented, 939 new samples added\n",
      "1550 samples augmented, 946 new samples added\n",
      "1560 samples augmented, 955 new samples added\n",
      "1570 samples augmented, 958 new samples added\n",
      "1580 samples augmented, 971 new samples added\n",
      "1590 samples augmented, 985 new samples added\n",
      "1600 samples augmented, 992 new samples added\n",
      "1610 samples augmented, 994 new samples added\n",
      "1620 samples augmented, 998 new samples added\n",
      "1630 samples augmented, 1005 new samples added\n",
      "1640 samples augmented, 1010 new samples added\n",
      "1650 samples augmented, 1018 new samples added\n",
      "1660 samples augmented, 1027 new samples added\n",
      "1670 samples augmented, 1039 new samples added\n",
      "1680 samples augmented, 1048 new samples added\n",
      "1690 samples augmented, 1060 new samples added\n",
      "1700 samples augmented, 1066 new samples added\n",
      "1710 samples augmented, 1072 new samples added\n",
      "1720 samples augmented, 1076 new samples added\n",
      "1730 samples augmented, 1077 new samples added\n",
      "1740 samples augmented, 1083 new samples added\n",
      "1750 samples augmented, 1092 new samples added\n",
      "1760 samples augmented, 1097 new samples added\n",
      "1770 samples augmented, 1107 new samples added\n",
      "1780 samples augmented, 1111 new samples added\n",
      "1790 samples augmented, 1115 new samples added\n",
      "1800 samples augmented, 1123 new samples added\n",
      "1810 samples augmented, 1129 new samples added\n",
      "1820 samples augmented, 1131 new samples added\n",
      "1830 samples augmented, 1136 new samples added\n",
      "1840 samples augmented, 1144 new samples added\n",
      "1850 samples augmented, 1149 new samples added\n",
      "1860 samples augmented, 1165 new samples added\n",
      "1870 samples augmented, 1171 new samples added\n",
      "1880 samples augmented, 1184 new samples added\n",
      "1890 samples augmented, 1190 new samples added\n",
      "1900 samples augmented, 1193 new samples added\n",
      "1910 samples augmented, 1200 new samples added\n",
      "1920 samples augmented, 1202 new samples added\n",
      "1930 samples augmented, 1203 new samples added\n",
      "1940 samples augmented, 1206 new samples added\n",
      "1950 samples augmented, 1208 new samples added\n",
      "1960 samples augmented, 1209 new samples added\n",
      "1970 samples augmented, 1217 new samples added\n",
      "1980 samples augmented, 1229 new samples added\n",
      "1990 samples augmented, 1233 new samples added\n",
      "2000 samples augmented, 1237 new samples added\n",
      "2010 samples augmented, 1243 new samples added\n",
      "2020 samples augmented, 1248 new samples added\n",
      "2030 samples augmented, 1257 new samples added\n",
      "2040 samples augmented, 1265 new samples added\n",
      "2050 samples augmented, 1275 new samples added\n",
      "2060 samples augmented, 1282 new samples added\n",
      "2070 samples augmented, 1289 new samples added\n",
      "2080 samples augmented, 1295 new samples added\n",
      "2090 samples augmented, 1298 new samples added\n",
      "2100 samples augmented, 1305 new samples added\n",
      "2110 samples augmented, 1314 new samples added\n",
      "2120 samples augmented, 1318 new samples added\n",
      "2130 samples augmented, 1328 new samples added\n",
      "2140 samples augmented, 1335 new samples added\n",
      "2150 samples augmented, 1339 new samples added\n",
      "2160 samples augmented, 1341 new samples added\n",
      "2170 samples augmented, 1350 new samples added\n",
      "2180 samples augmented, 1353 new samples added\n",
      "2190 samples augmented, 1361 new samples added\n",
      "2200 samples augmented, 1367 new samples added\n",
      "2210 samples augmented, 1373 new samples added\n",
      "2220 samples augmented, 1391 new samples added\n",
      "2230 samples augmented, 1395 new samples added\n",
      "2240 samples augmented, 1409 new samples added\n",
      "2250 samples augmented, 1415 new samples added\n",
      "2260 samples augmented, 1419 new samples added\n",
      "2270 samples augmented, 1427 new samples added\n",
      "2280 samples augmented, 1436 new samples added\n",
      "2290 samples augmented, 1441 new samples added\n",
      "2300 samples augmented, 1447 new samples added\n",
      "2310 samples augmented, 1452 new samples added\n",
      "2320 samples augmented, 1463 new samples added\n",
      "2330 samples augmented, 1469 new samples added\n",
      "2340 samples augmented, 1478 new samples added\n",
      "2350 samples augmented, 1486 new samples added\n",
      "2360 samples augmented, 1498 new samples added\n",
      "2370 samples augmented, 1503 new samples added\n",
      "2380 samples augmented, 1509 new samples added\n",
      "2390 samples augmented, 1513 new samples added\n",
      "2400 samples augmented, 1517 new samples added\n",
      "2410 samples augmented, 1531 new samples added\n",
      "2420 samples augmented, 1532 new samples added\n",
      "2430 samples augmented, 1542 new samples added\n",
      "2440 samples augmented, 1549 new samples added\n",
      "2450 samples augmented, 1556 new samples added\n",
      "2460 samples augmented, 1557 new samples added\n",
      "2470 samples augmented, 1563 new samples added\n",
      "2480 samples augmented, 1570 new samples added\n",
      "2490 samples augmented, 1574 new samples added\n",
      "2500 samples augmented, 1576 new samples added\n",
      "2510 samples augmented, 1576 new samples added\n",
      "2520 samples augmented, 1576 new samples added\n",
      "2530 samples augmented, 1576 new samples added\n",
      "2540 samples augmented, 1576 new samples added\n",
      "2550 samples augmented, 1576 new samples added\n",
      "2560 samples augmented, 1576 new samples added\n",
      "2570 samples augmented, 1576 new samples added\n",
      "2580 samples augmented, 1576 new samples added\n",
      "2590 samples augmented, 1576 new samples added\n",
      "2600 samples augmented, 1576 new samples added\n",
      "2610 samples augmented, 1576 new samples added\n",
      "2620 samples augmented, 1576 new samples added\n",
      "2630 samples augmented, 1576 new samples added\n",
      "2640 samples augmented, 1576 new samples added\n",
      "2650 samples augmented, 1576 new samples added\n",
      "2660 samples augmented, 1576 new samples added\n",
      "2670 samples augmented, 1576 new samples added\n",
      "2680 samples augmented, 1576 new samples added\n",
      "2690 samples augmented, 1576 new samples added\n",
      "2700 samples augmented, 1576 new samples added\n",
      "2710 samples augmented, 1576 new samples added\n",
      "2720 samples augmented, 1576 new samples added\n",
      "2730 samples augmented, 1576 new samples added\n",
      "2740 samples augmented, 1576 new samples added\n",
      "2750 samples augmented, 1576 new samples added\n",
      "2760 samples augmented, 1576 new samples added\n",
      "2770 samples augmented, 1576 new samples added\n",
      "2780 samples augmented, 1576 new samples added\n",
      "2790 samples augmented, 1576 new samples added\n",
      "2800 samples augmented, 1576 new samples added\n",
      "2810 samples augmented, 1576 new samples added\n",
      "2820 samples augmented, 1576 new samples added\n",
      "2830 samples augmented, 1576 new samples added\n",
      "2840 samples augmented, 1576 new samples added\n",
      "2850 samples augmented, 1576 new samples added\n",
      "2860 samples augmented, 1576 new samples added\n",
      "2870 samples augmented, 1576 new samples added\n",
      "2880 samples augmented, 1576 new samples added\n",
      "2890 samples augmented, 1576 new samples added\n",
      "2900 samples augmented, 1576 new samples added\n",
      "2910 samples augmented, 1576 new samples added\n",
      "2920 samples augmented, 1576 new samples added\n",
      "2930 samples augmented, 1576 new samples added\n",
      "2940 samples augmented, 1576 new samples added\n",
      "2950 samples augmented, 1576 new samples added\n",
      "2960 samples augmented, 1576 new samples added\n",
      "2970 samples augmented, 1576 new samples added\n",
      "2980 samples augmented, 1576 new samples added\n",
      "2990 samples augmented, 1576 new samples added\n",
      "3000 samples augmented, 1576 new samples added\n",
      "3010 samples augmented, 1587 new samples added\n",
      "3020 samples augmented, 1590 new samples added\n",
      "3030 samples augmented, 1596 new samples added\n",
      "3040 samples augmented, 1605 new samples added\n",
      "3050 samples augmented, 1612 new samples added\n",
      "3060 samples augmented, 1616 new samples added\n",
      "3070 samples augmented, 1629 new samples added\n",
      "3080 samples augmented, 1638 new samples added\n",
      "3090 samples augmented, 1641 new samples added\n",
      "3100 samples augmented, 1649 new samples added\n",
      "3110 samples augmented, 1654 new samples added\n",
      "3120 samples augmented, 1658 new samples added\n",
      "3130 samples augmented, 1663 new samples added\n",
      "3140 samples augmented, 1672 new samples added\n",
      "3150 samples augmented, 1678 new samples added\n",
      "3160 samples augmented, 1686 new samples added\n",
      "3170 samples augmented, 1693 new samples added\n",
      "3180 samples augmented, 1698 new samples added\n",
      "3190 samples augmented, 1702 new samples added\n",
      "3200 samples augmented, 1709 new samples added\n",
      "3210 samples augmented, 1795 new samples added\n",
      "3220 samples augmented, 1871 new samples added\n",
      "3230 samples augmented, 1920 new samples added\n",
      "3240 samples augmented, 2064 new samples added\n",
      "3250 samples augmented, 2162 new samples added\n",
      "3260 samples augmented, 2298 new samples added\n",
      "3270 samples augmented, 2342 new samples added\n",
      "3280 samples augmented, 2476 new samples added\n",
      "3290 samples augmented, 2627 new samples added\n",
      "3300 samples augmented, 2629 new samples added\n",
      "3310 samples augmented, 2715 new samples added\n",
      "3320 samples augmented, 2807 new samples added\n",
      "3330 samples augmented, 2872 new samples added\n",
      "3340 samples augmented, 2920 new samples added\n",
      "3350 samples augmented, 3026 new samples added\n",
      "3360 samples augmented, 3089 new samples added\n",
      "3370 samples augmented, 3187 new samples added\n",
      "3380 samples augmented, 3275 new samples added\n",
      "3390 samples augmented, 3359 new samples added\n",
      "3400 samples augmented, 3472 new samples added\n",
      "3410 samples augmented, 3539 new samples added\n",
      "3420 samples augmented, 3550 new samples added\n",
      "3430 samples augmented, 3557 new samples added\n",
      "3440 samples augmented, 3559 new samples added\n",
      "3450 samples augmented, 3565 new samples added\n",
      "3460 samples augmented, 3578 new samples added\n",
      "3470 samples augmented, 3588 new samples added\n",
      "3480 samples augmented, 3594 new samples added\n",
      "3490 samples augmented, 3595 new samples added\n",
      "3500 samples augmented, 3600 new samples added\n",
      "3510 samples augmented, 3607 new samples added\n",
      "3520 samples augmented, 3607 new samples added\n",
      "3530 samples augmented, 3607 new samples added\n",
      "3540 samples augmented, 3607 new samples added\n",
      "3550 samples augmented, 3607 new samples added\n",
      "3560 samples augmented, 3701 new samples added\n",
      "3570 samples augmented, 3891 new samples added\n",
      "3580 samples augmented, 4018 new samples added\n",
      "3590 samples augmented, 4118 new samples added\n",
      "3600 samples augmented, 4262 new samples added\n",
      "3610 samples augmented, 4366 new samples added\n",
      "3620 samples augmented, 4503 new samples added\n",
      "3630 samples augmented, 4530 new samples added\n",
      "3640 samples augmented, 4530 new samples added\n",
      "3650 samples augmented, 4530 new samples added\n",
      "3660 samples augmented, 4530 new samples added\n",
      "3670 samples augmented, 4530 new samples added\n",
      "3680 samples augmented, 4530 new samples added\n",
      "3690 samples augmented, 4530 new samples added\n",
      "3700 samples augmented, 4530 new samples added\n",
      "3710 samples augmented, 4530 new samples added\n",
      "3720 samples augmented, 4530 new samples added\n",
      "3730 samples augmented, 4530 new samples added\n",
      "3740 samples augmented, 4530 new samples added\n",
      "3750 samples augmented, 4530 new samples added\n",
      "3760 samples augmented, 4530 new samples added\n",
      "3770 samples augmented, 4530 new samples added\n",
      "3780 samples augmented, 4530 new samples added\n",
      "3790 samples augmented, 4530 new samples added\n",
      "3800 samples augmented, 4530 new samples added\n",
      "3810 samples augmented, 4530 new samples added\n",
      "3820 samples augmented, 4530 new samples added\n",
      "3830 samples augmented, 4530 new samples added\n",
      "3840 samples augmented, 4530 new samples added\n",
      "3850 samples augmented, 4530 new samples added\n",
      "3860 samples augmented, 4530 new samples added\n",
      "3870 samples augmented, 4530 new samples added\n",
      "3880 samples augmented, 4530 new samples added\n",
      "3890 samples augmented, 4530 new samples added\n",
      "3900 samples augmented, 4530 new samples added\n",
      "3910 samples augmented, 4530 new samples added\n",
      "3920 samples augmented, 4530 new samples added\n",
      "3930 samples augmented, 4530 new samples added\n",
      "3940 samples augmented, 4530 new samples added\n",
      "3950 samples augmented, 4530 new samples added\n",
      "3960 samples augmented, 4530 new samples added\n",
      "3970 samples augmented, 4530 new samples added\n",
      "3980 samples augmented, 4530 new samples added\n",
      "3990 samples augmented, 4530 new samples added\n",
      "4000 samples augmented, 4530 new samples added\n",
      "4010 samples augmented, 4530 new samples added\n",
      "4020 samples augmented, 4530 new samples added\n",
      "4030 samples augmented, 4530 new samples added\n",
      "4040 samples augmented, 4530 new samples added\n",
      "4050 samples augmented, 4530 new samples added\n",
      "4060 samples augmented, 4530 new samples added\n",
      "4070 samples augmented, 4530 new samples added\n",
      "4080 samples augmented, 4530 new samples added\n",
      "4090 samples augmented, 4530 new samples added\n",
      "4100 samples augmented, 4530 new samples added\n",
      "4110 samples augmented, 4530 new samples added\n",
      "4120 samples augmented, 4530 new samples added\n",
      "4130 samples augmented, 4530 new samples added\n",
      "4140 samples augmented, 4530 new samples added\n",
      "4150 samples augmented, 4530 new samples added\n",
      "4160 samples augmented, 4530 new samples added\n",
      "4170 samples augmented, 4530 new samples added\n",
      "4180 samples augmented, 4530 new samples added\n",
      "4190 samples augmented, 4601 new samples added\n",
      "4200 samples augmented, 4680 new samples added\n",
      "4210 samples augmented, 4776 new samples added\n",
      "4220 samples augmented, 4892 new samples added\n",
      "4230 samples augmented, 4966 new samples added\n",
      "4240 samples augmented, 5063 new samples added\n",
      "4250 samples augmented, 5140 new samples added\n",
      "4260 samples augmented, 5206 new samples added\n",
      "4270 samples augmented, 5299 new samples added\n",
      "4280 samples augmented, 5401 new samples added\n",
      "4290 samples augmented, 5403 new samples added\n",
      "4300 samples augmented, 5404 new samples added\n",
      "4310 samples augmented, 5406 new samples added\n",
      "4320 samples augmented, 5409 new samples added\n",
      "4330 samples augmented, 5409 new samples added\n",
      "4340 samples augmented, 5411 new samples added\n",
      "4350 samples augmented, 5413 new samples added\n",
      "4360 samples augmented, 5416 new samples added\n",
      "4370 samples augmented, 5419 new samples added\n",
      "4380 samples augmented, 5423 new samples added\n",
      "4390 samples augmented, 5426 new samples added\n",
      "4400 samples augmented, 5428 new samples added\n",
      "4410 samples augmented, 5433 new samples added\n",
      "4420 samples augmented, 5436 new samples added\n",
      "4430 samples augmented, 5437 new samples added\n",
      "4440 samples augmented, 5437 new samples added\n",
      "4450 samples augmented, 5440 new samples added\n",
      "4460 samples augmented, 5443 new samples added\n",
      "4470 samples augmented, 5445 new samples added\n",
      "4480 samples augmented, 5447 new samples added\n",
      "4490 samples augmented, 5450 new samples added\n",
      "4500 samples augmented, 5452 new samples added\n",
      "4510 samples augmented, 5454 new samples added\n",
      "4520 samples augmented, 5457 new samples added\n",
      "4530 samples augmented, 5460 new samples added\n",
      "4540 samples augmented, 5461 new samples added\n",
      "4550 samples augmented, 5462 new samples added\n",
      "4560 samples augmented, 5462 new samples added\n",
      "4570 samples augmented, 5462 new samples added\n",
      "4580 samples augmented, 5462 new samples added\n",
      "4590 samples augmented, 5462 new samples added\n",
      "4600 samples augmented, 5462 new samples added\n",
      "4610 samples augmented, 5462 new samples added\n",
      "4620 samples augmented, 5462 new samples added\n",
      "4630 samples augmented, 5462 new samples added\n",
      "4640 samples augmented, 5462 new samples added\n",
      "4650 samples augmented, 5462 new samples added\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "df = data_aug(df, classes_count_dct, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fantasy': 1023, 'science': 1023, 'crime': 1023, 'history': 991, 'horror': 996, 'thriller': 1023, 'psychology': 1023, 'romance': 1023, 'sports': 1023, 'travel': 971}\n"
     ]
    }
   ],
   "source": [
    "classes_count_dct = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['genre'] in classes_count_dct:\n",
    "        classes_count_dct[row['genre']] += 1\n",
    "    else:\n",
    "        classes_count_dct[row['genre']] = 1\n",
    "\n",
    "print(classes_count_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X = vectorizer.fit_transform(df['text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8095, 10000), (2024, 10000), (8095,), (2024,))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb = LabelEncoder()\n",
    "y = lb.fit_transform(df['genre'])\n",
    "\n",
    "X = X.astype('float32')\n",
    "y = y.astype('float32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2, shuffle=True)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', mode='auto', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5, min_lr=0.0000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 32)                320032    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320490 (1.22 MB)\n",
      "Trainable params: 320426 (1.22 MB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(classes.shape[0], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "64/64 [==============================] - 1s 7ms/step - loss: 1.2532 - accuracy: 0.6315 - val_loss: 2.1108 - val_accuracy: 0.7544 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.3947 - accuracy: 0.9248 - val_loss: 1.9572 - val_accuracy: 0.8320 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.1961 - accuracy: 0.9671 - val_loss: 1.7492 - val_accuracy: 0.8804 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.1289 - accuracy: 0.9774 - val_loss: 1.4990 - val_accuracy: 0.8903 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0939 - accuracy: 0.9821 - val_loss: 1.1581 - val_accuracy: 0.8918 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.0800 - accuracy: 0.9838 - val_loss: 0.8328 - val_accuracy: 0.8943 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.0665 - accuracy: 0.9839 - val_loss: 0.5669 - val_accuracy: 0.8948 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.0561 - accuracy: 0.9872 - val_loss: 0.4239 - val_accuracy: 0.8948 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.0559 - accuracy: 0.9853 - val_loss: 0.3701 - val_accuracy: 0.8948 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "57/64 [=========================>....] - ETA: 0s - loss: 0.0483 - accuracy: 0.9864\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.0484 - accuracy: 0.9864 - val_loss: 0.3609 - val_accuracy: 0.8938 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.0460 - accuracy: 0.9870 - val_loss: 0.3690 - val_accuracy: 0.8953 - lr: 5.0000e-04\n",
      "Epoch 12/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.0377 - accuracy: 0.9884 - val_loss: 0.3792 - val_accuracy: 0.8923 - lr: 5.0000e-04\n",
      "Epoch 13/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0392 - accuracy: 0.9872 - val_loss: 0.3849 - val_accuracy: 0.8972 - lr: 5.0000e-04\n",
      "Epoch 14/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0370 - accuracy: 0.9879 - val_loss: 0.3888 - val_accuracy: 0.8948 - lr: 5.0000e-04\n",
      "Epoch 15/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0331 - accuracy: 0.9876 - val_loss: 0.3965 - val_accuracy: 0.8948 - lr: 5.0000e-04\n",
      "Epoch 16/50\n",
      "57/64 [=========================>....] - ETA: 0s - loss: 0.0336 - accuracy: 0.9881\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0342 - accuracy: 0.9879 - val_loss: 0.4028 - val_accuracy: 0.8958 - lr: 5.0000e-04\n",
      "Epoch 17/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0331 - accuracy: 0.9891 - val_loss: 0.4007 - val_accuracy: 0.8933 - lr: 2.5000e-04\n",
      "Epoch 18/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0315 - accuracy: 0.9884 - val_loss: 0.4025 - val_accuracy: 0.8933 - lr: 2.5000e-04\n",
      "50/64 [======================>.......] - ETA: 0s - loss: 0.3928 - accuracy: 0.8963"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/anaconda3/envs/aienv/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3849 - accuracy: 0.8972\n",
      "64/64 [==============================] - 0s 1ms/step\n",
      "[0.384943425655365, 0.8972331881523132]\n"
     ]
    }
   ],
   "source": [
    "output = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                   epochs=epochs, batch_size=batch_size,\n",
    "                   callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "path = './model.h5'\n",
    "model.save(path)\n",
    "\n",
    "eval = model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
